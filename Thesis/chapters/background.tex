\chapter{Background}
\textit{\Cref{sec:grovers_explanation} is adapted from parts of my previous work \cite{ownwork}, with revisions and additions for clarity.}

\section{Grover's Algorithm for Solving Unstructured Search}\label{sec:grovers_explanation}
% Explain the principles of Grover’s algorithm. You should
% explain the following points:
% ▶ The problem the algorithm solves.
% ▶ The relevant circuit diagram.
% ▶ How the Grover iteration operates.
% ▶ The complexity of the algorithm.

% INTRODUCTION
% What are we talking about here? What is the gist of it?

% What is the status? Why do we need cryptography?
% Symmetric cryptography is a thing, and the general strategy for beating it is doing a brute-force search. 
% Grover's algorithm does this. Let's explore in more detail.

% Link the cryptographical motivation to the formal description
Formally, the computational problem that Grover's algorithm addresses is a quite general problem known as \emph{unstructured search} \cite{Qiskit2023}. It can be described as follows:
% \noindent\fcolorbox{black}{lightgray}{%
%     \parbox{\textwidth}{%
%         Let $\Sigma$ denote the binary alphabet $\{0,1\}$. Suppose we are given a function $f: \Sigma^n \rightarrow \Sigma$ that we can compute efficiently. We wish to determine the binary string $x \in \Sigma^n$ such that $f(x) = 1$, or ``no solution" if no such $x$ exists.
%     }%
% }
\begin{center}
    \begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.85\textwidth}
            Let $\Sigma$ denote the binary alphabet $\{0,1\}$. Suppose we are given a function $f: \Sigma^n \rightarrow \Sigma$ that we can compute efficiently. We wish to determine the binary string $x \in \Sigma^n$ such that $f(x) = 1$, or ``no solution'' if no such $x$ exists.
        \end{minipage}
    };
    \node[fancytitle, right=10pt] at (box.north west) {The unstructured search problem};
    \end{tikzpicture}%
\end{center}
%\vspace{0.5cm}
We call this search problem \emph{unstructured} because $f$ can be an arbitrary function, meaning there is no underlying pattern or \emph{promise} \cite{Qiskit2023} that we can take advantage of to systematically search for the solution string $x$, if it even exists. This means that in the classical approach to solving this problem, we can be sure of the solution only after checking every possible string (of which there are $2^n$), thus requiring $2^n$ queries. As we will see, Grover's algorithm offers a quadratic improvement over classical algorithms, meaning the number of operations needed when we use this quantum algorithm is on the order of the square-root of the number of operations required when solving the same problem classically.
% DIAGRAM
% This is the diagram. This is what the different parts do.
\begin{center}
    \begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.85\textwidth}
        The algorithm essentially consists of three main parts: we have initialization of $n$ qubits into an equal superposition (1), followed by $t$ Grover iterates (2), and finally measurement to reveal a candidate solution (3). These three stages are displayed and labeled in the circuit diagram below:
        
            \begin{quantikz}
    \slice{1}            & \gate{H} \slice{2} & \gate[3]{\mathcal{G}} & \gate[3]{\mathcal{G}} & \ \ldots \ & \gate[3]{\mathcal{G}} \slice{3} & \meter{}\\
    \lstick{\ket{0^n}} & \gate{H}           &                       &                       & \ \ldots \ &                                 & \meter{}\\ 
                         & \gate{H}           &                       &                       & \ \ldots \ &                                 & \meter{}
\end{quantikz}
        \end{minipage}
    };
    \node[fancytitle, right=10pt] at (box.north west) {Circuit diagram of Grover's algorithm};
    \end{tikzpicture}%
\end{center}

%What do we do?
% Where are we making use of quantum phenomena to reduce computational time?
Let's dive into the algorithm. Our first step is using the \emph{Hadamard gate} (denoted by $H$) \cite{quantumgates} on each of the $n$ input qubits. $H$ has no classical equivalent, and using it allows us to take advantage of a strictly quantum phenomenon known as \emph{superposition} \cite{superposition}. This means the qubit can evaluate to either 0 or 1, and we can apply processing to it as if the qubit holds both values simultaneously. Thus, in finding a binary string of length $n$, using the Hadamard gate on $\ket{0^{n}}$ lets us evaluate all $2^n$ strings in $\Sigma^n$ concurrently. For convenience, we will use the letter $N$ to refer to the value $2^n$ from this point on.

% Specifically, how does the iteration process work? How do we know when to stop, and what happens if we don't?
Next is the Grover iteration step, represented by sequential application of the Grover operator $\mathcal{G}$ in the diagram. Its definition and circuit diagram are explained in detail on the next page\supervisors{Should I reference this more formally?}. 
\begin{center}
    \begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.85\textwidth}
        The gate sequence needed for the Grover operation $\mathcal{G}$ is:
\[H^{\otimes n} Z_{\text{OR}} H^{\otimes n} Z_f \]
where $Z_f$ is the phase query gate for the function $f$, and $Z_{\text{OR}}$ is the phase query gate for the $n$-bit logical OR function. They are defined as follows:
\begin{align*}
    % H &= \frac{1}{\sqrt{2}}\begin{bmatrix}
    %     1 & 1 \\
    %     1 & -1
    % \end{bmatrix}
    Z_f&: \ket{x} \mapsto (-1)^{f(x)}\ket{x} \hspace{0.6cm} \forall x \in \Sigma^n \\
    Z_{\text{OR}}&: \begin{cases}
        \ket{x} \hspace{0.5cm} x = 0^n \hspace{0.6cm} \forall x \in \Sigma^n\\
        -\ket{x} \hspace{0.2cm} x \neq 0^n \hspace{0.6cm} \forall x \in \Sigma^n
    \end{cases}
\end{align*}
Since the gates are applied from right to left, we apply $Z_f$ first, followed by $H$, then $Z_{\text{OR}}$, and finally another set of Hadamard gates. The circuit diagram of the Grover operation on three bits looks like this:

\hspace{2.3cm}
            \begin{quantikz}
            &\gate[3]{Z_f} & \gate{H} & \gate[3]{Z_{\text{OR}}} & \gate{H} & \\
             &               & \gate{H} &                         & \gate{H} & \\
              &              & \gate{H} &                         & \gate{H} &
\end{quantikz}

Concerning evaluation of the circuit, only the $Z_f$ gate requires a query to $f$. Thus, the number of iterations $t$ we perform with $\mathcal{G}$ corresponds to how many queries to $f$ we need.
        \end{minipage}
    };
    \node[fancytitle, right=10pt] at (box.north west) {The Grover operation $\mathcal{G}$};
    \end{tikzpicture}%
\end{center}
A natural way to use the algorithm for search problems is to choose a number of iterations $t$ of the Grover operation, run the circuit to obtain a candidate solution $x$, and check if $x$ is a solution (in this case whether $f(x) = 1$). If so, we return $x$, otherwise we run the algorithm again (possibly with a different value for $t$) or return ``no solution''. This sounds simple enough, but still, important questions remain unanswered: What is the ``suitable'' number of operations $t$, how do we identify it, and why must it be chosen so precisely?

To understand what is happening here, it is helpful to separate the possible solution strings in $\Sigma^n$ into two categories, which we will call solutions and nonsolutions. These sets can be described as follows:
\begin{align*}
    A_0 &= \{x \in \Sigma^n: f(x) = 0\}\\
    A_1 &= \{x \in \Sigma^n: f(x) = 1\}
\end{align*}
where $A_0$ contains all the strings that evaluate to 0 when given as input to $f$, and $A_1$ has the strings we seek that evaluate to 1. In particular, we are interested in uniform superpositions over these two sets\footnote{Importantly, we will assume from here on that $A_0$ and $A_1$ are both nonempty. However, we can still easily see what happens in the last two cases (all strings evaluate to 0, or all strings are solutions), as the former will return no solution, and in the latter case, any string is a solution. Since we are able to handle these special cases, we can safely continue our analysis with this assumption.}, defined as:
\begin{align*}
    \ket{A_0} &= \frac{1}{\sqrt{|A_0|}}\sum_{x \in A_0} \ket{x} \\
    \ket{A_1} &= \frac{1}{\sqrt{|A_1|}}\sum_{x \in A_1} \ket{x}
\end{align*}
An important observation here is the fact that we can write the initialized uniform superposition as a linear combination of the states $\ket{A_0}$
and $\ket{A_1}$:
\[H^{\otimes n} \ket{0^n} = \sqrt{\frac{|A_0|}{N}}\ket{A_0} + \sqrt{\frac{|A_1|}{N}}\ket{A_1}\]

The key idea we utilize is this: \textbf{The state of the qubit register remains in the subspace spanned by} $\boldsymbol{\ket{A_0}}$
\textbf{and} $\boldsymbol{\ket{A_1}}$ \textbf{after every application of} $\boldsymbol{\mathcal{G}}$. Thus, we would like to process the initialized state $H^{\otimes n} \ket{0^n}$ (with uniform amplitude across all $N$ components) such that through successive iterations, we nudge the amplitude of the solution component $\ket{A_1}$ slightly higher, while lowering the amplitude of the nonsolutions $\ket{A_0}$. As we know, in a quantum state vector, the amplitude of any component squared represents the probability of seeing that component after measurement. Provided we find the suitable number of iterations $t$, we can push the amplitude of the state corresponding to the solutions arbitrarily high, and consequently see a high probability that this state is measured.

\textcite{Qiskit2023} calculates the application of $\mathcal{G}$ on the $\ket{A_0}$ and $\ket{A_1}$, resulting in the following linear combinations:
\begin{align*}
    \mathcal{G}\ket{A_0} &= \frac{|A_0|- |A_1|}{N}\ket{A_0} + \frac{2\sqrt{|A_0| \cdot |A_1|}}{N}\ket{A_1} \\
    \mathcal{G}\ket{A_1} &= -\frac{2\sqrt{|A_0| \cdot |A_1|}}{N}\ket{A_0} + \frac{|A_0|- |A_1|}{N}\ket{A_1}
\end{align*}
This clearly demonstrates that we are still in the span of $\ket{A_0}$ and $\ket{A_1}$ after an application of $\mathcal{G}$. Moreover, we can describe the action of $\mathcal{G}$ as a rotation matrix, and by once again following the example of \textcite{Qiskit2023}, we can also rewrite it as the square of a somewhat simpler-looking matrix:
\[
\begin{bmatrix}
    \frac{|A_0|- |A_1|}{N} & -\frac{2\sqrt{|A_0| \cdot |A_1|}}{N} \\
      \frac{2\sqrt{|A_0| \cdot |A_1|}}{N}   & \frac{|A_0|- |A_1|}{N}
\end{bmatrix} = 
\begin{bmatrix}
    \sqrt{\frac{|A_0|}{N}} & -\sqrt{\frac{|A_1|}{N}}\\
    \sqrt{\frac{|A_1|}{N}} & \sqrt{\frac{|A_0|}{N}}
\end{bmatrix}^2
\]
Now it is easier to see the similarity to the general form of a rotation matrix, with $\theta = \sin ^{-1} \left (\sqrt{\frac{|A_1|}{N}} \right )$: 
\[
\begin{bmatrix}
    \sqrt{\frac{|A_0|}{N}} & -\sqrt{\frac{|A_1|}{N}}\\
    \sqrt{\frac{|A_1|}{N}} & \sqrt{\frac{|A_0|}{N}}
\end{bmatrix} = \begin{bmatrix}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta
\end{bmatrix}
\]
Since squaring a rotation is the same thing as applying it twice, we return to the original rotation matrix for $\mathcal{G}$, and write it as:
\[
\begin{bmatrix}
    \frac{|A_0|- |A_1|}{N} & -\frac{2\sqrt{|A_0| \cdot |A_1|}}{N} \\
      \frac{2\sqrt{|A_0| \cdot |A_1|}}{N}   & \frac{|A_0|- |A_1|}{N}
\end{bmatrix} = \begin{bmatrix}
    \cos 2\theta & -\sin 2\theta \\
    \sin 2\theta & \cos 2\theta
\end{bmatrix}
\]
with $\theta$ defined as before. Thus, we have found a rotation matrix that describes the action of $\mathcal{G}$ on the subspace spanned by $A_0$ and $A_1$. This also means that the state of the qubit register after initialization can be described as:
\[
H^{\otimes n} \ket{0^n} = \sqrt{\frac{|A_0|}{N}}\ket{A_0} + \sqrt{\frac{|A_1|}{N}}\ket{A_1} = \cos \theta \ket{A_0} + \sin \theta \ket{A_1}
\]
So, each time we apply $\mathcal{G}$, the state of the register is rotated by $2\theta$, and we can find a closed-form expression for the state after $t$ applications of $\mathcal{G}$. For brevity, we will henceforth refer to the initialized qubit register $H^{\otimes n} \ket{0^n}$ as $\ket{u}$.
\[
    % \ket{u} &= \cos \theta \ket{A_0} + \sin \theta \ket{A_1}\\
    % \mathcal{G} \ket{u} &= \cos 3\theta \ket{A_0} + \sin 3\theta \ket{A_1}\\
    % \mathcal{G}^2 \ket{u} &= \cos 5\theta \ket{A_0} + \sin 5\theta \ket{A_1}\\
    % \vdots\\
    \mathcal{G}^t \ket{u} = \cos ((2t+1)\theta) \ket{A_0} + \sin ((2t+1)\theta) \ket{A_1}
\]
This expression makes sense intuitively: $2\theta$ for each application of $\mathcal{G}$, and one additional $\theta$ to account for the state we started in.

Now recall that for any quantum state of the 
form $\alpha\ket{A_0} + \beta \ket{A_1}$, we measure a solution $x \in A_1$ with probability $|\beta|^2$. With the state in question $\mathcal{G}^t \ket{u} = \cos ((2t+1)\theta) \ket{A_0} + \sin ((2t+1)\theta) \ket{A_1}$, the probability of measuring a solution after $t$ iterations is $\sin^2((2t+1)\theta)$. This is the probability we want to maximize in order to find a solution to the search problem with Grover's algorithm, and as such, we call $\ket{A_1}$ our \emph{target state} \cite{Qiskit2023}. 

Another consideration is that we would also like to minimize $t$, as this is the number of queries to $f$ we have to make. Thus, to reach our target state with a probability close to 1 while minimizing $t$, we aim to find $t$ such that $2(t+1)\theta \approx \frac{\pi}{2}$, since this is the smallest angle where the square of the sine function equals 1. In terms of $t$, we aim for $t\approx\frac{\pi}{4\theta} - \frac{1}{2}$. We cannot guarantee that this value of $t$ is an integer, so we find the number of iterations $t$ by choosing $t = \lfloor \frac{\pi}{4\theta} \rfloor$. For the algorithm to work properly, it is also important that we do not iterate past the recommended iteration number, lest we avoid ``overshooting'' our solution by rotating past it. If we simply continue applying $\mathcal{G}$, we will in fact decrease the probability that the solution state is measured, and over time, we will oscillate between solutions and nonsolutions as we rotate the register state around the unit circle \cite{Qiskit2023}.

Up to this point, we have studied a general version of Grover's algorithm that can be applied to a number of search problems, ranging from those we know to have a single solution, a larger solution set of a fixed size, or even an unknown number of solutions. We have determined that an iteration number of $t = \lfloor \frac{\pi}{4\theta} \rfloor$ (and no more) will maximize the probability of finding one of these solutions, but what we must also consider is the fact that this way of setting iteration number $t$ also depends on how many solutions there are, as $\theta$ depends on the size of the set $A_1$. Fortunately, since we are considering only the cryptographic application of using the algorithm to search for a single key from some symmetric cryptographic protocol, we can restrict ourselves to the cases where we know there is exactly one solution (or one cryptographic key).

\emph{Unique search}, as this problem is formally known, is equivalent to the general unstructured search problem, with the additional promise that there is exactly one solution string $x$. Recall that in the general case, $\theta = \sin ^{-1} \left (\sqrt{\frac{|A_1|}{N}} \right )$. Now that we have determined there is only a single solution, we know that $|A_1| = 1$. Thus, for large $N$, we see that $\theta = \sin ^{-1} \left (\sqrt{\frac{1}{N}} \right ) \approx \sqrt{\frac{1}{N}}$, since for smaller angles, the sine function starts to look more like the identity function \cite{Qiskit2023}. If we use this approximation of $\theta$ in our previously determined expression $t = \lfloor \frac{\pi}{4\theta} \rfloor$, we get $t = \lfloor \frac{\pi}{4}\sqrt{N} \rfloor$. Since $t$ represents queries to the function $f$, we see the quadratic improvement mentioned earlier: Rather than $N$ queries, we are now on the order of the square root of that. 

\subsection{Practical Application and Limitations of Grover's Algorithm}
At first glance, the improvements afforded by Grover's algorithm seem substantial. Though not a perfect algorithm in the sense that there is a probabilistic element and some chance of error\footnote{in contrast to the classical runtime we discussed, where the exhaustive search strategy leaves no chance of error whatsoever}, Grover's clearly has an advantage in runtime complexity over the classical strategy. Therefore, we identify that there is some trade-off between computational efficiency and certainty of outcome when choosing to use Grover's to tackle the unstructured search problem. In terms of correctness, it can be proven analytically that probability of success when using Grover's is always greater than or equal to $1 - \frac{1}{N}$ \cite{Qiskit2023}, meaning success is typically likely, particularly as $N$ grows large. We are also free to run the algorithm in its entirety as many times as we choose, and if any one of them succeeds, it will identify a solution, with a high chance of doing so using fewer queries to $f$ than its classical counterpart. With the classical cost on the order of $2^{128}$ for AES-128, a naive calculation finds the new cost with Grover's square-root speedup to be around $2^{64}$, perhaps with some small constant to account for overhead and setting up the quantum circuit. However, this estimation does not take into account a number of practical concerns and considerations.

First, Grover's algorithm, as described in \cref{sec:grovers_explanation}, uses an oracle $f$, which in our case corresponds to the AES protocol. AES, a complicated protocol in and of itself, must be implemented in the quantum hardware using computationally expensive gates and structures \cite{jaques2024ches_presentation,chen2025quantumcircuitimplementingaes}, which we introduce and discuss in more detail in \cref{sec:universalgates}. This adds significant cost to the runtime of the algorithm \cite{jaques2024ches_presentation}, which the commonly cited Big O abstraction does not capture \cite{bigO}. Exploring circuit designs of AES and their costs is outside of the scope of this project, but it is an active field of research \cite{resourcefficientAES,cryptoeprint,chen2025quantumcircuitimplementingaes}
that will be crucial for full asymptotic analysis and practical implementation of Grover's algorithm applied to AES.

Secondly, as pointed out by Jacques, any realistic attack will require parallelization \cite{jaques2024ches_presentation}, and unfortunately, Grover's algorithm does not parallelize well. If we consider the full key space of $N$ keys and partition it into $P$ subsets, we can assign each of the $P$ partitions to a machine responsible for a search space containing $\frac{N}{P}$ keys. With the square-root speedup over the classical method as discussed, we can now find the key in $\mathcal{O}(\sqrt{\frac{N}{P}})$ time. When comparing this to serial Grover's with a runtime of $\mathcal{O}(\sqrt{N})$, we find that parallelizing only reduces the runtime by a factor of $\sqrt{P}$, not $P$. Worse yet, the additional partitions add an overhead, with the total cost of operations going up to $P \cdot \mathcal{O}(\sqrt{\frac{N}{P}}) = \mathcal{O}(\sqrt{P \cdot N})$. Thus, in our case with AES-128, the bad parallelism of Grover means its cost is certainly higher than the naive estimation of $2^{64}$. 

The parallelization argument holds for any algorithm on the order of $\sqrt{N}$, and it turns out $\mathcal{O}(\sqrt{N})$ is the best we can do: For the general unstructured search problem, it has been proven that Grover's algorithm is \emph{asymptotically optimal} \cite{Bennett_1997, Zalka_1999}, meaning that as the search space grows, it is not possible to find an algorithm that performs better than on the order of $\sqrt{N}$ in the worst case. Despite its clear limitations, Grover's is thus still a contender to break the security of AES and similar symmetric encryption schemes by quantum computing, motivating the exploration we undertake in this project.
\supervisorsinline{I'm trying to introduce some sort of motivation for working with Grover's despite the pessimistic outlook we discussed in this section, but this last sentence might still be a little unfounded. Thoughts?}

\section{Quantum Error Correction for Stable Quantum Memory}
As we briefly touched upon, future quantum computers will certainly require some form of error correction, as quantum systems are prone to noise, causing logical errors during computation. Unlike in a classical computer where the only errors we consider are bit-flip errors (where 0 is interpreted as 1, or vice versa), quantum information can be destroyed by two separate types of errors: bit-flips and phase-flips. This, in combination with quantum hardware being inherently more difficult to work with from an engineering perspective, has the unfortunate result of massively complicating the circuitry required for computation as soon as any practically viable error correction measures are introduced, a topic we will revisit in more detail in \cref{sec:universalgates}. %Delving further into this issue will be the main focus of this project.

\subsection{Toric Surface Code}
The standard method of error correction that is used in all existing quantum computers built for fault-tolerance \supervisors{This is my impression, but it's hard to find a specific source.} is the \emph{surface code}. Following the discovery of the toric code\footnote{There is some inconsistency in the literature about what constitutes a \emph{toric code}, a \emph{surface code}, and a \emph{toric surface code}. We adhere to the nomenclature set by IBM \cite{ibm_quantum_other_code_families}, where toric codes exist in a grid across the surface of a torus with periodic boundary conditions, and surface codes and toric surface codes are used interchangeably to refer specifically to the portion of a torus grid laid flat with non-periodic boundary conditions.} in 1997 \cite{Kitaev_2003}, toric surface code has remained a prevalent error correcting architecture because it is based on simple principles and maintains geometric locality\footnote{The only links required in the surface code are between neighboring qubits, as opposed to long-distance connections required by certain other codes that are more challenging from an engineering standpoint.}.

In essence, surface code works by creating a grid of data qubits and measurement qubits interspersed and defining \emph{stabilizers} that flag bit-flip ($X$) errors and phase-flip ($Z$) errors when they occur in specific locations. The error correcting capability lies in measuring the stabilizers across the grid repeatedly, decoding where errors have occured, and applying the appropriate correction based on the stabilizer measurement. The stabilizer formalism can in many ways be viewed as the quantum generalization of parity checks \cite{paritycheckstabilizerwiki,pesah2023_stabilizer_trilogy1}, which are often used similarly in classical error correction to identify where and how to restore corrupted data.

\begin{figure}[h!]
    \centering
    \includegraphics{figures/quantumrep-07-00025-g001.png}
    \caption{A surface code grid with physical data qubits located on the edges, demonstrating a plaquette ($Z$ stabilizer) in blue and a vertex ($X$ stabilizer) in red. Although only one of each is shown here, the entire grid is covered in these stabilizers in a repeating pattern.\\
    Image source: \textcite{quantum7020025}}\label{fig:stabilizerdiagram}
\end{figure}

In \cref{fig:stabilizerdiagram}\supervisors{I might remake this graphic myself.}, we show how taking four qubits surrounding a vertex or a square face in the grid (known as a \emph{plaquette}) defines an $X$ or $Z$ stabilizer respectively. Check qubits for measurements in the stabilizers are a crucial element of quantum error correction specifically, as measuring the actual state of data qubits to identify data corruption would destroy their states, and thus the data itself. 

The stabilizer formalism allows us to define algebraically the units described visually in \cref{fig:stabilizerdiagram} with two sets of commuting stabilizer generators. For each vertex $v$ in the grid, we have an $X$ stabilizer
\[S^X_v = \prod_{j \in \text{star}(v)} X_j,\]
which multiplies the Pauli $X$ operator on the four\footnote{or fewer at the grid boundaries} edges touching $v$. We similarly define a $Z$ stabilizer for each plaquette $p$ as
\[S^Z_p = \prod_{j \in \text{boundary}(p)} Z_j,\]
which multiplies the Pauli $Z$ operator on the edges surrounding $p$. The groups formed by all $S^X$ and $S^Z$ respectively have elements that are all tensor products of Pauli operators and that commute. Together, each of these groups has a shared eigenspace constrained by 
\[S_i \ket{\psi} = + \ket{\psi} \hspace{0.5cm} \forall i,\]
where $S_i$ represents a general stabilizer (either $X$ or $Z$) applied to an appropriate vertex or plaquette, denoted by $i$. This eigenspace contains the legal code words, or in this case, the states corresponding to logical 0 and logical 1 (henceforth denoted by $\ket{0}_L$ and $\ket{1}_L$ respectively).

The Pauli operators satisfy the anticommutation relation $\sigma_j \sigma_k + \sigma_k \sigma_j  = 2\delta_{jk} \mathbb{I}_2$, where $\sigma_i$ are the Pauli operators according to the naming convention $\sigma_1 = X, \sigma_2 = Y, \sigma_3 = Z$, $\delta_{jk}$ is the Kronecker delta and $\mathbb{I}_2$ is the $2 \times 2$ identity matrix \cite{paulimatrices}. Thus, if an error operator $E$ commutes with a stabilizer $S_i$, the eigenvalue of the surrounding qubits' state is unchanged:
\[
S_i(E\ket{\psi}) = E(S_i\ket{\psi}) = +E\ket{\psi}.
\]
However, if $E$ and $S_i$ anticommute, we will have $S_i(E\ket{\psi}) = -E(S_i\ket{\psi})$ instead, which changes the eigenvalue of the local stabilizer state from +1 to $-1$. 

At this point, we can do a measurement of the ancillary check qubits of the stabilizers to determine which ones violate our constraints and flag errors in their neighbors. Thinking back to the notion of parity checks, we consider the result of each stabilizer measurements a remapping of the parity check values 0 and 1 to +1 and $-1$ respectively. This is a very convenient mapping for us, because when we work with $Z$ operators\footnote{A measurement of $X$ is slightly more involved, but still unproblematic. The relation $HXH = Z$ allows us to simply apply $H$ and do a measurement of $Z$ instead.}, $\pm 1$ happen to be the eigenvalues of the eigenstates $\ket{0}$ and $\ket{1}$. Thus, $\pm 1$ represent a way for us to access information about the state without measuring the state itself. In other words, if we measure the check qubits in the stabilizers and see $\ket{0}$, we know the data qubits are in the eigenstate +1, and we conclude no error has occurred among the neighboring qubits of this stabilizer\footnote{Strictly speaking, we only know an even number of errors has occurred. For instance, two errors in the neighboring qubits of the stabilizer will also flip its state back into the eigenstate +1, and thus be undetectable.}. However, if we see $\ket{1}$, we know the surrounding data qubits are in the eigenstate $-1$, and an error has occurred here. Together, the eigenvalues $s_i$ that we observe at each of the $m$ stabilizer measurements form a string $(s_1, s_2, \dots, s_{m-1}, s_m) \in \{\pm1\}^m$, called the \emph{syndrome} of the full state. It tells us about what errors are likely to have occurred and where, which we can use to determine what corrections we would like to apply to restore the logical state.
%This approach can be used to identify where errors occur, which we can apply the mostly likely stabilizer correction chain to.
%This writeup --> https://github.com/mcclow12/Quantum-Computing-Surface-Code-Simulation/blob/master/writeup.pdf explains surface code pretty well. May be worth testing out their Jupyter notebook too. 

While conceptually simple, the toric surface code has a clear drawback in that it is very qubit-hungry, with estimates predicting a ratio around a 1000 to 1 of physical to logical qubits required for realistic error correction rates \cite{jaques2024ches_presentation,Gidney2025}. This has sparked further study and innovation in order to uncover more favorable schemes in terms of resource efficiency.

\subsection{Alternatives to Toric Surface Code}
The continued usage of the surface code at the current correction rates is one of the underlying assumptions in Jacques' predictions. Thus, it is interesting for the sake of our analysis to see how the prognosis for Grover's algorithm changes with a different choice of error correcting code. A few promising examples can be found in the \emph{Low Density Parity Check} (LDPC) family of codes, of which the surface code is a special case\supervisors{Do I need a source for this, and if so, what?}. In particular, \emph{Bivariate Bicycle code}, also known as BB code, is a strong potential candidate \cite{Bravyi2024}. Though similar to surface codes at a high level of abstraction, BB codes differ in that their stabilizers are not geometrically local, and has therefore been called ``surface code with extra long-distance checks'' \cite{sommerstudentNSM}. These checks allow for a number of advantages over the surface code, such as a higher threshold for error correction and lower qubit overhead \cite{Bravyi2024}. 

%Following the example of \textcite{sommerstudentNSM}, 
We begin by defining the following shift matrices:
\[x = P_{\ell} \otimes \mathbb{I}_m, \hspace{0.3cm} y = \mathbb{I}_{\ell} \otimes P_{m},\]
where $\mathbb{I}_m$ is the $m \times m$\supervisors{Can I use $m$ here when I used $m$ to mean something else previously? I'm running out of letters!} identity matrix and $P$ is a cyclic permutation matrix in the form $\sum_{i=1}^{\ell} \ket{i}\bra{i+1}$, which maps $\ket{i+1}$ to $\ket{i}$ \cite{sommerstudentNSM}. In addition, $x$ and $y$ satisfy $xy = yx$ and $x^{\ell} = y^m = \mathbb{I}_{\ell \times m}$. We use $x$ and $y$ to represent stepping through the grid in the $x$- and $y$-direction respectively.

Formally, BB code is defined as a pair of $\ell m \times \ell m$ binary matrices $A(x, y), B(x,y)$ that are sums of our shift matrices $x$ and $y$ to integer powers. These matrix polynomials are then used to define parity check matrices $H^X$ and $H^Z$:
\[H^X = [A | B], \hspace{0.3cm} H^Z = [B^\intercal | A^\intercal].\]
The dimension of $H^X$ and $H^Z$ is thus $\ell m \times 2 \ell m$. For our purposes, $2 \ell m = n$, the number of physical qubits, and $ \ell m = \frac{n}{2}$, which is the number of vertex and plaquette stabilizers in the grid. 
We use these two matrices to index which qubits in the grid are to be acted on by stabilizers. Each row contains the qubit indices for a specific stabilizer, and each column is a binary indication of whether a specific qubit in the grid is part of that stabilizer. For instance, in BB code, each row will therefore have six 1s, as each stabilizer acts on six qubits, while the surface code in this notation would have four 1s per row (at most), because each stabilizer acts on only four qubits. The shifting mechanism defined by $A$ and $B$ is the mathematical way to describe the systematic construction of an evenly-spaced pattern repeating throughout the grid. With some appropriate indexing of the grid, we are able to specify the exact qubits to include in each stabilizer, uniquely defining a BB code.

Measuring a stabilizer corresponds to checking whether the associated parity condition set in the $H^X$ and $H^Z$ matrices is satisfied. Specifically, we require that
\[
H^X (H^Z)^\intercal = 0.\]
This requirement ensures that the stabilizers commute, so that we can apply similar logic as with the standard surface code to identify the stabilizers for which this constraint does not hold, where the anticommutation relation will let us detect stabilizers in the $-1$ eigenstate. The syndrome that is produced informs the corrections we make as a result.

\supervisorsinline{I'm thinking about adding more details about the decoding process for surface code and/or BB code, which would go here for BB code. Is that necessary?}

It is common to use the notation $[[n, k, d]]$ to specify the parameters of an error correcting code, where $n$ is the number of physical qubits, $k$ is the number of logical qubits we encode, and $d$ is the number of errors the code can correct. IBM, the developer of the BB code, has primarily considered a BB code they call the \emph{gross} code\footnote{after the term for a dozen dozen, known as a gross}, which is a [[144, 12, 12]]-code (though many other variants exist and are perhaps better suited for our purposes –– see \cref{chap:methods}). In this specification, 144 physical qubits are required to encode 12 qubits, and the code can correct up to 12 errors without issue \cite{Bravyi2024}. With the 1000 to 1 ratio needed for the toric surface code, the gross code (and BB codes in general) offer a clear improvement in this regard. As an additional plus, the connections between qubits in the BB code form a Tanner graph \cite{tannergraphwiki}, which is a bipartite graph that can be separated into two disjoint sets. In this structure, nodes (qubits) within each set are only connected to members of the other, never to one another. This can be useful when physically constructing this architecture, since it allows the hardware to be split cleanly into two separate levels.

%Python code for simulating BB codes can be found here: \url{https://github.com/AntonBrekke/BBCODE-QEC-2025/tree/main}.
\supervisorsinline{This is where I'll make a subsection discussing tile codes as well if I end up including them. At the moment I'm not sure I'll have time to look into them, so I'll leave it at BB codes.}
% Tile code somewhat resembles the toric surface code, and are also related to BB codes. However, it does not have  periodic boundary conditions. It has the stabilizers located outside of a ``tile'' of qubits, instead of interspersed between. 

% They have created an algorithm which helps you select which qubits to check (?). 

\section{Universal Gate Sets for Quantum Computation}\label{sec:universalgates}
Assuming we have created some scheme for fault-tolerant storage of quantum data, we still face a number of significant challenges before we can perform any fault-tolerant computations. In particular, the \emph{Eastin-Knill theorem} \cite{Eastin_2009} states the following:
\begin{center}
    \begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.85\textwidth}
            For any nontrivial local-error-detecting quantum code, the set of logical unitary product operators is not universal.
        \end{minipage}
    };
    \node[fancytitle, right=10pt] at (box.north west) {The Eastin-Knill theorem};
    \end{tikzpicture}%
\end{center}

\todo[inline]{I think the key here is that the logical unitary product operators are specifically related to what operators can be expressed and worked with in the stabilizer formalism, which as we know the $T$-gates cannot.}

To understand what this means, we need to to delve further into how applying gates to quantum states forms a circuit. Mathematically, we can represent any quantum circuit on $n$ qubits with a single unitary matrix\footnote{A unitary matrix, also known as simply a unitary, is a norm-preserving matrix $U \in \mathbb{C}^N \times  \mathbb{C}^N, N \in \mathbb{N}$ whose conjugate transpose $U^\dagger$ is also its inverse: $U^\dagger U = UU^\dagger = \mathbb{I}_N$.}
\[U \in \mathbb{C}^{2^n \times 2^n}.\]
While this representation can certainly be useful, like in classical simulations with very few qubits, collapsing the circuit into one matrix in this way generally does not capture a lot of important behaviors and phenomena we are interested in when studying quantum error correction. In hardware, we never apply a single giant unitary gate; instead, as we have seen, we decompose the computation into smaller, simpler gates. In doing so, we could in principle have our pick from a large selection of possible gates, but if for simplicity we restrict ourselves to implementing only a few different types, we would like the set of gates we choose to be \emph{universal}. A universal gate set can approximate any such unitary $U$ to an arbitrary degree of precision with a finite sequence of gates, an important property for our gate set to have, as without it, there will always be computations we cannot perform.

Put another way, the theorem tells us that we cannot have a universal gate set where every gate is \emph{transversal}, meaning a gate that can transform a logical qubit by applying a gate to each of the $n$ physical qubits\supervisors{What is a good source to use for the definition of transversal?}:
\[G_L = \bigotimes^n_{i=1} G_i.
\]
Additionally, transversal gates ensure single-qubit errors do not propagate to multiple qubits within an error correcting patch, making them inherently fault-tolerant: A single error occurring when applying the gate will produce at most one error for the error-correcting patch to correct. This is clearly a desirable property for gates in our chosen gate set to have, but the Eastin-Knill theorem describes a fundamental limitation, in that any gate set we choose capable of approximating arbitrary quantum circuits will contain at least one nontransversal gate. 

\todo[inline]{What is the link between transversality (a logical gate is applied by applying the same gate to its constituent physical qubits individually) and being able to manipulate the stabilizers only in order to alter the logical state the patch contains? By which I mean, why is it that there exists a way for me to mess with the stabilizers and achieve the effect of an X, Y, Z or H gate, but not the T gate? And I know rationally that Eastin-Knill is the answer, since at least one gate is hard, but I don't have a conceptual understanding of why there isn't some manipulation of stabilizers corresponding to the application of the T gate.}

\todo[inline]{\textit{ChatGPT's answer:} The clean way to see the connection is to separate three layers: (i) the stabilizer group that \emph{defines} the code space, (ii) logical Pauli operators as nontrivial elements of the normalizer of that stabilizer group, and (iii) logical gates as automorphisms of the code space. In the surface (toric) code, the stabilizers are local $X$-type stars and $Z$-type plaquettes, and the logical operators are noncontractible $X$- and $Z$-strings. A logical unitary is implementable by “manipulating stabilizers” precisely when it corresponds to a symmetry or locality-preserving transformation of this Pauli/stabilizer structure.

Transversality fits naturally into this picture. A transversal gate acts independently on each physical qubit, so conjugation by the global unitary reduces to conjugation of each local Pauli. If the single-qubit gate maps Pauli operators to Pauli operators (as Clifford gates do), then each stabilizer generator—being a product of Paulis—gets mapped to another Pauli product. In other words, the stabilizer group is mapped to itself (possibly with relabeling), and the code space is preserved. That is why Clifford gates are compatible with transversal implementations in CSS codes like the surface code: their defining algebraic property is that they normalize the Pauli group.

When you ``manipulate stabilizers'' in the surface code—via code deformation, lattice symmetries, boundary twists, or relabeling $X \leftrightarrow Z$—you are again performing operations that map stabilizer generators to stabilizer generators. Conceptually, you are implementing an automorphism of the stabilizer group. Any such automorphism necessarily induces a transformation on the logical Pauli operators, and because the logical Pauli operators generate the logical Pauli group, the induced logical gate must be a Clifford. This is the essential restriction: stabilizer-preserving transformations can only produce logical operations that send Paulis to Paulis.

Now consider the $T$ gate. Conjugation by $T$ does not preserve the Pauli group: for example,
\[
T X T^\dagger = \frac{1}{\sqrt{2}}(X + Y),
\]
which is not a Pauli operator but a coherent superposition of Paulis. This already tells you something fundamental. If you tried to implement logical $T$ purely by ``messing with stabilizers'', you would need a transformation that sends a logical Pauli operator to a non-Pauli operator while still mapping the stabilizer group to itself. But stabilizers, by definition, are Pauli products. Any operation that preserves the stabilizer formalism must map Pauli products to Pauli products under conjugation. There is simply no room inside that algebraic structure to accommodate the non-Pauli images produced by $T$.

From a more geometric/topological perspective, the surface code is a two-dimensional topological stabilizer code. Any locality-preserving logical gate corresponds to a topological symmetry of the anyon model underlying the code. In the surface code, those symmetries form exactly the Clifford group. The $T$ gate would require a transformation that mixes topological charge sectors in a way that cannot be realized by a locality-preserving deformation of the lattice or stabilizers. So it is not just that ``at least one gate must be hard'' (Eastin–Knill); it is that the entire stabilizer/topological framework only supports Clifford automorphisms.

Conceptually, then, the answer is this: manipulating stabilizers works precisely when the desired logical gate is an automorphism of the Pauli/stabilizer structure. Clifford gates have that property; $T$ does not. Because $T$ pushes you outside the Pauli algebra under conjugation, it cannot arise from any transformation that merely reshuffles or locally deforms stabilizers. That algebraic incompatibility is the deep reason you cannot realize $T$ by stabilizer manipulation alone.
}

\subsection{Choices to Make When Performing Quantum Computations}
\supervisorsinline{Does this section (perhaps rewritten somewhat) fit better in the methods section?}
There are a few separate challenges with doing operations on quantum data that we must address in order to reach our goal of a fault-tolerant circuit for Grover's algorithm. Assuming we have made some choice of error correcting code, first, \ding{192} we must choose an appropriate universal gate set. Next, \ding{193} we need to take the operations we want and determine an equivalent calculation using only the gates in our chosen gate set. Finally, we face another problem entirely in taking our chosen gate set to the quantum hardware and \ding{194} figuring out how the circuit we designed will be implemented on the error-correcting patches corresponding to logical qubits. 

With regards to \ding{192}, \emph{Clifford+T} is a common choice of universal gate set, fittingly comprised of the \emph{Clifford} and \emph{$T$-gates}. Clifford gates are unitaries that map Pauli operators to Pauli operators under conjugation, which makes them very well-suited as logical gates in LDPC codes like the ones we have seen. Applying Clifford gates to codes based on $X$ and $Z$ stabilizers respects the stabilizer structure and code space, and since they can be implemented by performing single-qubit operations across all qubits, they are also transversal. The possibility of these simple, single-qubit operations not only grants Clifford gates transversality and built-in fault-tolerance, but it also means they are considered computationally easy to implement. 

However, the $T$-gates, given by
\[
T = \begin{pmatrix}
1 & 0 \\
0 & e^{i\pi/4}
\end{pmatrix},
\]
are a different story entirely. A requirement for universality, these gates are neither transversal nor easy computationally; owing to the Eastin-Knill restriction, they necessitate complicated workarounds to be implemented fault-tolerantly (which we return to in \cref{sec:phasequerygates}). For now, Jacques' argument fundamentally relies on the assumption that we will encode the Grover circuit with the Clifford+T gate set, where the number of T-gates is ultimately the determining factor in his estimation of the time complexity of the algorithm. However, if there were a better way to create the circuit that requires less T-gates (or T-gate equivalents), we could possibly see substantial improvements, since the huge estimated cost of implementing T-gates is a major contributor to why Grover's algorithm works out so poorly in practice.

\supervisorsinline{If it becomes at all interesting to explore other options besides the Clifford+T gate set, I will define and discuss them here. Otherwise, I will elaborate a bit and justify my choice of sticking to Clifford+T.}

As for the next challenge, while \ding{193} is not necessarily easy in isolation, provided we have our operation expressed as a circuit in some other gate set, there fortunately exist tools such as Synthetiq \cite{synthetiq2025} that can do this optimization and search process for us. In our case with Grover's algorithm, the circuit with freely chosen gates is well known, and Synthetiq can be used to convert it into whatever other limited gate set we would like.%\textcolor{blue}{Additionally, the Grover circuit is quite small in the case we are simulating, so it may be fine to just do this manually, or not at all. We should bear in mind the hardware-aware approach though to avoid doing calculations that cancel.}

The issue that this project boils down to in many ways is therefore \ding{194}. While certain logical operators are known or can be identified by the software tools, others might pose more of a challenge. As highlighted earlier in \cref{sec:grovers_explanation}\supervisors{Might add a figure name here}, the gates we need to perform the full circuit for Grover's algorithm are $H$, $Z_f$ and $Z_{\text{OR}}$. Of these, $H$ is largely considered computationally easy, taking our standard $X$ and $Z$ corrections and swapping them, since $H$ maps $\ket{0}$ to $\ket{+}$, $\ket{1}$ to $\ket{-}$, and vice versa, leaving us in the realm of the computationally easy Clifford gates. The phase query gates $Z_f$ and $Z_{\text{OR}}$ are more problematic, however, and will require the computationally hard T-gates.

\section{Phase Query Gates in Error Correcting Architecture}\label{sec:phasequerygates}
\todo[inline]{TBA after I finish coding this and figure out how it works, because this is where the issues with T-gates arise. Will touch upon T-gate implementation in more detail, magic state distillation for instance.}
