\chapter{Background}
\section{What is Grover's algortihm?}
\textcolor{red}{Include content from TEK5550 text on Grover's algorithm here.}

\section{What is Jacques' full line of argumentation?}
In the normal case when we would use Grover's algorithm, we assume no structure. The argument for there being security in this case is that the number of potential keys is so large that we would not be able to do a brute-force search attack on a classic computer in any reasonable amoutn of time (brute-force being the only option here beceause there is no structure to the problem or the way the keys are determined). The circuit for the quantum algorithm Grover's is such that we may have a speedup on the order of the square root. We also know (from my own previous work) that the runtime of Grover's scales with the number of Grover iterates, which themselves scale with the number of queries to the arbitrary funciton $f$, or the AES algorithm in our case, which is needed to see if a binary string $x$ is a solution. Each Grover iteration requires running the entire AES encryption circuit, and this is the costly part.

There might also be constant factors here we don't know about, so to see the full picture, we need to know exactly how our $f$, namely AES, will be implemented in a circuit. Jacques provides a table of estimates of different values on page 20 of the PDF (slide 8 in the deck). The three quantities of apparent interest are T gates (simple, computationally easy gates), Clifford gates (a different type of gate that requires a huge number of qubits) and the depth, which I take to mean the number of gates that have to be applied serially, kind of functioning as a proxy for time (as they cannot be run in parallel). Finally, we have an estimate of a couple thousand logical qubits required for each size of AES in the table, but of course there could be millions of qubits hiding behind this number.

We definitely need some form of error correction, since quantum computations are very prone to noise. But we are orders of magnitude off the number of physical qubits we need in order to be anywhere near the estimated number of necessary logical qubits.

At this point, he introduces NIST's 2017 MAXDEPTH metric as a baseline for how many logical gates/operations the current quantum computing architectures perform serially over certain periods of time, like a year ($2^{40}$), a decade ($2^{64}$) or a millenium ($2^96$)\footnote{He also points out that this limit does not reflect decoherence concerns, i.e. the quantum state collapsing and quantum data disappearing.}. Additionally, he makes clear that any sort of realistic attack on AES would have to be parallelized. However, Grover doesn't parallelize well, as additional partitions add an overhead every time (simple calculation/explanation on slide 33 in the deck). We could argue that since the classical cost is on the order of $2^{128}$ for AES-128 and Grover offers a square root speedup, the new cost will be $2^{64}$, with some small constant to account for overhead and setting up the quantum circuit. However, we have now shown that the bad parallelism of Grover means it is not on the order of $2^{64}$, and we have no idea what the constant will be if we use a different type of code other than surface codes.

This paper will attempt to tackle just that. What happens if we use something else instead, and what might that be?

\section{Assumptions}
In this section, I will highlight a few different assumptions that Jacques makes. If we get rid of them, that might be a possible avenue of improving his prognosis.

\subsection{The attack is completely structureless}
What if the attack has more structure?\\
I think this is not the best path to go down as I believe this fundamentally alters the problem away from unstructured search.

\subsection{We use surface codes as a form of error correction at the current detection rate.}
Thomas has shown me a different type of promising error correction known as LDPC (Low Density Parity Check) codes, including BB codes and tile codes. These codes have a number of advantages over surface codes, such as a higher threshold for error correction and lower qubit overhead.\\

Why (or how) might this change the conclusion? Where in the argumentation does the assumption of surface codes have an impact?\\

I think investigating this is a good place to start.


\subsection{We encode the Grover circuit with Clifford + T gates}
This seems related to a really fundamental aspect of his argument, since the number of T-gates is what ultimately determines the time complexity of the algorithm. If there was a better way to create the circuit that requires less T-gates (or T-gate equivalents), we wouldn't be able to rely on his estimate, since the huge cost of the T-gates as he assumed is part of why this works out so poorly.

\section{Surface Code}
This writeup\footnote{\url{https://github.com/mcclow12/Quantum-Computing-Surface-Code-Simulation/blob/master/writeup.pdf}} explains surface code pretty well. May be worth testing out their Jupyter notebook too. 

Surface code works by creating a lattice of data qubits and measurement qubits interspersed. 
\textcolor{red}{Question: Where does the ratio ``1000:1'' come from? Seem like we need 4 per qubit????}

\section{Alternatives to Surface Code}
\subsection{Bivariate Bicycle Code}
\emph{Bivariate Bicycle code}, also known as BB code, is a quantum code in the Low Density Parity Check (LDPC) code class. Though similar to surface codes at a high level of abstraction, BB codes differ in that their check operators are not geometrically local, and has therefore been called ``surface code with extra long distance checks'' \textcolor{red}{(summer student project, p. 6)}. 

It is formally defined as a pair of binary matrices that are sums of shift matrices to integer powers.

Python code for simulating BB codes can be found here: \url{https://github.com/AntonBrekke/BBCODE-QEC-2025/tree/main}.

\subsection{Tile Code}
Tile code somewhat resembles the toric surface code, and are also related to BB codes. However, it does not have  periodic boundary conditions. It has the stabilizers located outside of a ``tile'' of qubits, instead of interspersed between. 

They have created an algorithm which helps you select which qubits to check (?). 